{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96958124-182d-45c6-a56f-f4ab8d0dfd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 10 04:12:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4500               On  | 00000000:82:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              18W / 200W |      2MiB / 20470MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ae795c-e4ad-4ec6-b5e8-3658c45ed2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2637b973-ce58-4c51-8c22-e64f0d31754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df263263-347b-4ad3-8b61-bd841cce3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae678c8-1a0e-40f1-9449-33916d9eab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_957/3482143200.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-10 04:12:10 config.py:549] Casting torch.bfloat16 to torch.float16.\n",
      "WARNING 02-10 04:12:10 config.py:177] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 02-10 04:12:10 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Mistral-7B-v0.1-AWQ', tokenizer='TheBloke/Mistral-7B-v0.1-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/workspace/', load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 02-10 04:12:12 weight_utils.py:164] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45347b0f0d3f4d8abce45dd7d98c92d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-10 04:13:03 llm_engine.py:322] # GPU blocks: 4816, # CPU blocks: 2048\n",
      "INFO 02-10 04:13:05 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-10 04:13:05 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-10 04:13:11 model_runner.py:698] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "#from prompts import questions\n",
    "#from rich.progress import track\n",
    "\n",
    "llm = LLM(\n",
    "    \"TheBloke/Mistral-7B-v0.1-AWQ\",\n",
    "    dtype=torch.float16,\n",
    "    quantization=\"AWQ\",    \n",
    "    download_dir=\"/workspace/\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd680a50-7787-448f-bc1b-df14e452bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afff3874-b3ad-4def-8f13-4ee66b82ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "out = llm.generate(\"This is me warming up the model\", sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6999c419-4b9d-47e3-bcfc-f213d4a51010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", I took some quite decent pictures of the model for one of my flight simulations a while back, so I started putting them together into a cute little thing (and title sequence...?)\\n\\nIf anyone has heard of or is even a part of the VTOL/SIM1000SANE area, you can use me as proof that not all the models/creations are much use and that you can be better than the rest of us.\\n\\nAnyway, a few comments:\\n\\n1) I have no idea how to contain the title screen within a single file, so go figure.\\n\\n2) My VCR was kinda acting up (see the blue and purple visual)\\n\\n3) My computer wasn't reacting very well because it seems like it wants an NTSC professor to Fren anim and I'm using NTSC. Obviously I don't know about video signals so if you're a little smarter than me\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out[0].prompt,\n",
    "out[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63c9b28-049d-4d1e-9fcc-49d0ce4dc794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running inference through 100 prompt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      "  1%|          | 1/100 [00:02<03:30,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:04<03:26,  2.11s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:05<03:04,  1.90s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:07<03:10,  1.98s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      "  5%|▌         | 5/100 [00:10<03:12,  2.03s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      "  6%|▌         | 6/100 [00:12<03:12,  2.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      "  7%|▋         | 7/100 [00:14<03:12,  2.07s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      "  8%|▊         | 8/100 [00:16<03:11,  2.08s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\u001b[A\n",
      "  9%|▉         | 9/100 [00:18<02:55,  1.93s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 10%|█         | 10/100 [00:20<02:58,  1.99s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\u001b[A\n",
      " 11%|█         | 11/100 [00:21<02:47,  1.88s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:23<02:51,  1.95s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:25<02:53,  2.00s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:27<02:41,  1.88s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:29<02:45,  1.95s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:31<02:47,  2.00s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:33<02:48,  2.03s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:36<02:48,  2.06s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:38<02:48,  2.08s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 20%|██        | 20/100 [00:40<02:47,  2.09s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\u001b[A\n",
      " 21%|██        | 21/100 [00:41<02:31,  1.92s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:43<02:34,  1.98s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:46<02:35,  2.02s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:48<02:35,  2.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:50<02:35,  2.07s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:51<02:23,  1.94s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:53<02:11,  1.81s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:54<02:01,  1.69s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:56<02:09,  1.82s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\u001b[A\n",
      " 30%|███       | 30/100 [00:58<01:59,  1.71s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 31%|███       | 31/100 [01:00<02:06,  1.83s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 32%|███▏      | 32/100 [01:02<02:10,  1.92s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\u001b[A\n",
      " 33%|███▎      | 33/100 [01:04<02:12,  1.98s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\u001b[A\n",
      " 34%|███▍      | 34/100 [01:06<02:02,  1.85s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\u001b[A\n",
      " 35%|███▌      | 35/100 [01:08<02:00,  1.86s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\u001b[A\n",
      " 36%|███▌      | 36/100 [01:10<02:04,  1.94s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\u001b[A\n",
      " 37%|███▋      | 37/100 [01:12<02:05,  2.00s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\u001b[A\n",
      " 38%|███▊      | 38/100 [01:14<01:57,  1.89s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\u001b[A\n",
      " 39%|███▉      | 39/100 [01:16<01:59,  1.96s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\u001b[A\n",
      " 40%|████      | 40/100 [01:18<02:00,  2.02s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 41%|████      | 41/100 [01:20<02:00,  2.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 42%|████▏     | 42/100 [01:22<02:00,  2.07s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 43%|████▎     | 43/100 [01:24<01:59,  2.09s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 44%|████▍     | 44/100 [01:26<01:58,  2.11s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 45%|████▌     | 45/100 [01:29<01:56,  2.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\u001b[A\n",
      " 46%|████▌     | 46/100 [01:30<01:45,  1.95s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 47%|████▋     | 47/100 [01:32<01:46,  2.00s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 48%|████▊     | 48/100 [01:34<01:46,  2.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 49%|████▉     | 49/100 [01:37<01:45,  2.08s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 50%|█████     | 50/100 [01:39<01:44,  2.10s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 51%|█████     | 51/100 [01:41<01:43,  2.11s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 52%|█████▏    | 52/100 [01:43<01:41,  2.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 53%|█████▎    | 53/100 [01:45<01:39,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 54%|█████▍    | 54/100 [01:47<01:38,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 55%|█████▌    | 55/100 [01:49<01:36,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 56%|█████▌    | 56/100 [01:52<01:34,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 57%|█████▋    | 57/100 [01:54<01:32,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 58%|█████▊    | 58/100 [01:56<01:29,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 59%|█████▉    | 59/100 [01:58<01:27,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 60%|██████    | 60/100 [02:00<01:25,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 61%|██████    | 61/100 [02:02<01:23,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\u001b[A\n",
      " 62%|██████▏   | 62/100 [02:04<01:16,  2.01s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 63%|██████▎   | 63/100 [02:06<01:15,  2.05s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\u001b[A\n",
      " 64%|██████▍   | 64/100 [02:08<01:10,  1.96s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 65%|██████▌   | 65/100 [02:10<01:10,  2.02s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 66%|██████▌   | 66/100 [02:12<01:09,  2.06s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 67%|██████▋   | 67/100 [02:14<01:08,  2.09s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 68/100 [02:16<01:07,  2.10s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 69%|██████▉   | 69/100 [02:19<01:05,  2.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 70%|███████   | 70/100 [02:21<01:03,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 71%|███████   | 71/100 [02:23<01:01,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 72%|███████▏  | 72/100 [02:25<00:59,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 73%|███████▎  | 73/100 [02:27<00:57,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 74%|███████▍  | 74/100 [02:29<00:55,  2.14s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 75%|███████▌  | 75/100 [02:32<00:53,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 76%|███████▌  | 76/100 [02:34<00:51,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 77%|███████▋  | 77/100 [02:36<00:49,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 78%|███████▊  | 78/100 [02:38<00:47,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 79%|███████▉  | 79/100 [02:40<00:45,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 80%|████████  | 80/100 [02:42<00:42,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 81%|████████  | 81/100 [02:44<00:40,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 82%|████████▏ | 82/100 [02:47<00:38,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 83%|████████▎ | 83/100 [02:49<00:36,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 84%|████████▍ | 84/100 [02:51<00:34,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 85%|████████▌ | 85/100 [02:53<00:32,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 86%|████████▌ | 86/100 [02:55<00:30,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 87%|████████▋ | 87/100 [02:57<00:27,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 88%|████████▊ | 88/100 [02:59<00:25,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 89%|████████▉ | 89/100 [03:02<00:23,  2.15s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\u001b[A\n",
      " 90%|█████████ | 90/100 [03:03<00:20,  2.02s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\u001b[A\n",
      " 91%|█████████ | 91/100 [03:05<00:18,  2.06s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 92%|█████████▏| 92/100 [03:08<00:16,  2.08s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 93%|█████████▎| 93/100 [03:10<00:14,  2.10s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 94%|█████████▍| 94/100 [03:12<00:12,  2.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\u001b[A\n",
      " 95%|█████████▌| 95/100 [03:14<00:10,  2.12s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 96%|█████████▌| 96/100 [03:16<00:08,  2.13s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [03:16<00:04,  1.57s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      " 98%|█████████▊| 98/100 [03:19<00:03,  1.74s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\u001b[A\n",
      " 99%|█████████▉| 99/100 [03:20<00:01,  1.72s/it]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\u001b[A\n",
      "100%|██████████| 100/100 [03:22<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens/sec:  93.677\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensor_parallel_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_per_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mtokens_generated \u001b[38;5;241m/\u001b[39m df\u001b[38;5;241m.\u001b[39mtime\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage tokens/sec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mtokens_per_sec\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm-benchmark-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mGPUs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_parallel_size' is not defined"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "questions = [\"This is me warming up the model\"]*100\n",
    "\n",
    "\n",
    "print(f\"running inference through {len(questions)} prompt.\")\n",
    "\n",
    "results = []\n",
    "for q in tqdm.tqdm(questions):\n",
    "    t0 = time.perf_counter()\n",
    "    output = llm.generate(q, sampling_params=sampling_params)[0]\n",
    "    t1 = time.perf_counter()\n",
    "    results.append(\n",
    "        {\"time\": t1 - t0, \"tokens_generated\": len(output.outputs[0].token_ids)}\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df[\"tokens_per_sec\"] = df.tokens_generated / df.time\n",
    "print(f\"Average tokens/sec: {df.tokens_per_sec.mean(): .3f}\")\n",
    "df.to_csv(f\"vllm-benchmark-{tensor_parallel_size}GPUs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e93070e-42d3-4c94-b41e-facb9e8b1998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tokens_generated</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.127778</td>\n",
       "      <td>200</td>\n",
       "      <td>93.994775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.096799</td>\n",
       "      <td>200</td>\n",
       "      <td>95.383477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.657019</td>\n",
       "      <td>158</td>\n",
       "      <td>95.351982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.101688</td>\n",
       "      <td>200</td>\n",
       "      <td>95.161592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.101913</td>\n",
       "      <td>200</td>\n",
       "      <td>95.151417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2.149532</td>\n",
       "      <td>200</td>\n",
       "      <td>93.043512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.263813</td>\n",
       "      <td>24</td>\n",
       "      <td>90.973508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.149723</td>\n",
       "      <td>200</td>\n",
       "      <td>93.035231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.658141</td>\n",
       "      <td>154</td>\n",
       "      <td>92.875069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.151485</td>\n",
       "      <td>200</td>\n",
       "      <td>92.959052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  tokens_generated  tokens_per_sec\n",
       "0   2.127778               200       93.994775\n",
       "1   2.096799               200       95.383477\n",
       "2   1.657019               158       95.351982\n",
       "3   2.101688               200       95.161592\n",
       "4   2.101913               200       95.151417\n",
       "..       ...               ...             ...\n",
       "95  2.149532               200       93.043512\n",
       "96  0.263813                24       90.973508\n",
       "97  2.149723               200       93.035231\n",
       "98  1.658141               154       92.875069\n",
       "99  2.151485               200       92.959052\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7715c2-5054-4241-a116-794625997846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_parallel_size = 1\n",
    "df.to_csv(f\"vllm-benchmark-{tensor_parallel_size}GPUs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dd63c-8928-4c06-9297-ec0a51ab4c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
